services:
  etcd:
    container_name: milvus-etcd
    image: quay.io/coreos/etcd:v3.5.18
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/etcd:/etcd
    command: etcd -advertise-client-urls=http://etcd:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    healthcheck:
      test: [ "CMD", "etcdctl", "endpoint", "health" ]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - app_network

  minio:
    container_name: milvus-minio
    image: minio/minio:RELEASE.2024-12-18T13-15-44Z
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    ports:
      - "9001:9001"
      - "9005:9000"
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/minio:/minio_data
    command: minio server /minio_data --console-address ":9001"
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9000/minio/health/live" ]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - app_network

  standalone:
    container_name: milvus-standalone
    image: milvusdb/milvus:v2.6.6
    command: [ "milvus", "run", "standalone" ]
    security_opt:
      - seccomp:unconfined
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
      MQ_TYPE: woodpecker
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/milvus:/var/lib/milvus
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9091/healthz" ]
      interval: 30s
      start_period: 90s
      timeout: 20s
      retries: 3
    ports:
      - "19530:19530"
      - "9091:9091"
    depends_on:
      - "etcd"
      - "minio"
    networks:
      - app_network

  frontend:
    container_name: frontend
    build:
      context: ./frontend
      dockerfile: docker/dev.Dockerfile

    environment:
      WATCHPACK_POLLING: true
      CHOKIDAR_USEPOLLING: true
    init: true
    env_file:
      - .env
      - .env.dev.local
    volumes:
      - ./frontend/src:/app/src
      - ./frontend/public:/app/public
    restart: unless-stopped
    ports:
      - 5173:5173
    networks:
      - app_network
    # depends_on:
    #   gateway:
    #     condition: service_healthy
    #   user-service:
    #     condition: service_healthy
    #   chat-service:
    #     condition: service_healthy
    
  api:
    container_name: api
    build:
      context: ./api
      dockerfile: docker/devPure.Dockerfile
    env_file:
        - .env
        - .env.dev.local
    ports:
      - 9000:9000
    volumes:
      - ./api:/app
    environment:
      - WATCHFILES_FORCE_POLLING=1  # This forces file watching (solves issues with hot reloading on windows)
      - WATCHFILES_POLLING_INTERVAL=1  # Check every 1 second
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app/app

      # ✅ GPU environment variables
      # - NVIDIA_VISIBLE_DEVICES=all
      # - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      # ✅ Force TensorFlow to use GPU
      # - CUDA_VISIBLE_DEVICES=0
    networks:
      - app_network
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [ gpu ]
    restart: unless-stopped
    
  eureka:
    container_name: eureka
    build:
      context: ./backend/eureka
      dockerfile: docker/dev.Dockerfile
    env_file:
      - .env
      - .env.dev.local
    ports:
      - 8761:8761
    volumes:
      - ./backend/eureka:/app
      - ~/.m2:/root/.m2
    networks:
      - app_network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8761/actuator/health || exit 1"]
      interval: 10s
      retries: 25
      start_period: 30s
      timeout: 5s
    depends_on:
      db:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy

  user-service:
    container_name: user-service
    build:
      context: ./backend/user-service
      dockerfile: docker/dev.Dockerfile
    env_file:
      - .env
      - .env.dev.local
    ports:
      - 8081:8081
    volumes:
      - ./backend/user-service:/app
      - ~/.m2:/root/.m2
    networks:
      - app_network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8081/actuator/health || exit 1"]
      interval: 10s
      retries: 25
      start_period: 60s
      timeout: 5s
    depends_on:
      eureka:
        condition: service_healthy
      # redis:
      #   condition: service_healthy 
  chat-service:
    container_name: chat-service
    build:
      context: ./backend/chat-service
      dockerfile: docker/dev.Dockerfile
    env_file:
      - .env
      - .env.dev.local
    ports:
      - 8082:8082
    volumes:
      - ./backend/chat-service:/app
      - ~/.m2:/root/.m2
    networks:
      - app_network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8082/actuator/health || exit 1"]
      interval: 10s
      retries: 25
      start_period: 60s
      timeout: 5s
    depends_on:
      eureka:
        condition: service_healthy
      # redis:
      #   condition: service_healthy   

  gateway:
    container_name: gateway
    build:
      context: ./backend/gateway
      dockerfile: docker/dev.Dockerfile
    env_file:
      - .env
      - .env.dev.local
    ports:
      - 8080:8080
    volumes:
      - ./backend/gateway:/app
      - ~/.m2:/root/.m2
    networks:
      - app_network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/actuator/health || exit 1"]
      interval: 10s
      retries: 25
      start_period: 40s
      timeout: 5s
    depends_on:
      - chat-service
      - user-service

      # chat-service:
        # condition: service_healthy
      # user-service:
        # condition: service_healthy
  
    # PostgreSQL container shared by multiple microservices:
    # - user-service uses database 'userdb'
    # - chat-service uses database 'chatdb'
    # This saves resources in development and testing environments
    # while keeping each service's data logically separated.asd
  db:
    container_name: db
    image: 'postgres:17'
    restart: unless-stopped
    env_file:
      - .env
      - .env.dev.local
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 10s
      retries: 10
      timeout: 5s
    ports:
      - '5432:5432'
    volumes:
      - "pgdata:/var/lib/postgresql/data"
      - ./backend/docker/postgres/init:/docker-entrypoint-initdb.d:ro
    networks:
      - app_network

  # redis:
  #   container_name: redis
  #   image: redis:8.0-alpine
  #   restart: unless-stopped
  #   ports:
  #     - "6379:6379"
  #   env_file:
  #    - .env
  #    - .env.dev.local
  #   command: ["redis-server", "--save", "", "--appendonly", "no", "--maxmemory", "256mb", "--maxmemory-policy", "allkeys-lru"]
  #   networks:
  #     - app_network
  #   healthcheck:
  #     test: ["CMD", "redis-cli", "ping"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 10
  #     start_period: 5s

  rabbitmq:
    container_name: rabbitmq
    image: rabbitmq:4.2-alpine
    restart: unless-stopped
    env_file:
      - .env
      - .env.dev.local
    healthcheck:
      test: ["CMD", "rabbitmqctl", "status"]
      interval: 10s
      retries: 10
      timeout: 5s
    ports:
      - "5672:5672"
    networks:
      - app_network
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq

volumes:
  pgdata:
  rabbitmq-data:
# Define a network, which allows containers to communicate
# with each other, by using their container name as a hostname
networks:
  app_network:
    driver: bridge
